{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"mushroom.csv\"\n",
    "\n",
    "'''''\n",
    "true_class: edible=0,poisonous=1\n",
    "neg_class: poisonous=0, edible=1\n",
    "cap-shape: bell=0,conical=1,convex=2,flat=3, knobbed=4,sunken=5\n",
    "cap-surface: fibrous=0,grooves=1,scaly=2,smooth=3\n",
    "cap-color: brown=0,buff=1,cinnamon=2,gray=3,green=4,pink=5,\n",
    "    purple=6,red=7,white=8,yellow=9\n",
    "odor: almond=0,anise=1,creosote=2,fishy=3,foul=4,musty=5,\n",
    "    none=6,pungent=7,spicy=8\n",
    "'''''\n",
    "\n",
    "# Read the csv file into a DataFrame object\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Convert DataFrame object into a numpy npdarray\n",
    "data = data.as_matrix()\n",
    "\n",
    "# Define sizes of training and validations sets\n",
    "train_size = 3000\n",
    "valid_size = 1000\n",
    "# Number of columns to use\n",
    "num_fields = 4\n",
    "\n",
    "# Choose a random subset\n",
    "indices = np.random.choice(8000, train_size)\n",
    "train_data = data[indices, :]\n",
    "\n",
    "indices = np.random.choice(8000, valid_size)\n",
    "valid_data = data[indices, :]\n",
    "# Seperate data from labels\n",
    "train_dataset = train_data[:, 2:]\n",
    "train_labels = train_data[:, :2]\n",
    "\n",
    "valid_dataset = valid_data[:, 2:]\n",
    "valid_labels = valid_data[:, :2]\n",
    "# Clear space in memory\n",
    "del data\n",
    "del valid_data\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of hidden layers\n",
    "num_nodes_1 = 1024\n",
    "num_nodes_2 = int(num_nodes_1 * 0.5)\n",
    "num_nodes_3 = int(num_nodes_1 * np.power(0.5, 2))\n",
    "# Number of possible outputs\n",
    "num_labels = 2\n",
    "# Batch size\n",
    "batch_size = 128\n",
    "# Beta for L2 regularization\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Tensorflow graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create weights and biases\n",
    "    def weights_and_biases(a, b):\n",
    "        w = tf.Variable(tf.truncated_normal(shape=[a, b], stddev=np.sqrt(2 / a)))\n",
    "        b = tf.Variable(tf.zeros([b]))\n",
    "        return w, b\n",
    "\n",
    "    # Create tensors for training data and labels and for\n",
    "    # validation data\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=[batch_size, num_fields])\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=[batch_size, num_labels])\n",
    "    tf_valid_data = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "\n",
    "    # Assign weights and biases\n",
    "    weights_1, biases_1 = weights_and_biases(num_fields, num_nodes_1)\n",
    "    weights_2, biases_2 = weights_and_biases(num_nodes_1, num_nodes_2)\n",
    "    weights_3, biases_3 = weights_and_biases(num_nodes_2, num_nodes_3)\n",
    "    weights_4, biases_4 = weights_and_biases(num_nodes_3, num_labels)\n",
    "\n",
    "    # Compute relu logits\n",
    "    def relu_logits(data, drop=False):\n",
    "        logits = tf.nn.relu(tf.matmul(data, weights_1) + biases_1)\n",
    "        # Are we going to drop some values\n",
    "        if drop:\n",
    "            logits = tf.nn.dropout(logits, 0.5)\n",
    "        logits = tf.nn.relu(tf.matmul(logits, weights_2) + biases_2)\n",
    "        if drop:\n",
    "            logits = tf.nn.dropout(logits, 0.5)\n",
    "        logits = tf.nn.relu(tf.matmul(logits, weights_3) + biases_3)\n",
    "        if drop:\n",
    "            logits = tf.nn.dropout(logits, 0.5)\n",
    "        logits = tf.matmul(logits, weights_4) + biases_4\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # Logits for training data\n",
    "    logits = relu_logits(tf_train_data, drop=True)\n",
    "\n",
    "    # Regular loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                  labels=tf_train_labels))\n",
    "    # L2 loss\n",
    "    reg = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3) + \\\n",
    "          tf.nn.l2_loss(weights_4)\n",
    "    loss = tf.reduce_mean(loss + reg * beta)\n",
    "\n",
    "    # Optimizer\n",
    "    start_learn_rate = 0.1\n",
    "    global_step = tf.Variable(0)\n",
    "    # Create a decaying learning rate\n",
    "    # start, global step, decay step, decay rate, staircase\n",
    "    learn_rate = tf.train.exponential_decay(start_learn_rate, global_step, 100000, 0.5, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "    # Training predictions\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    # Validation predictions\n",
    "    logits = relu_logits(tf_valid_data)\n",
    "    valid_predictions = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of training steps\n",
    "num_steps = 5001\n",
    "\n",
    "# Compute accuracy of predictions if %\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "# Create a Tensorflow session\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Deffine an offset\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "         # Create batch data and labels\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "        feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "        _, l, predictions = session.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_predictions.eval(),\n",
    "                                                                valid_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
